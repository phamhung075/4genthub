"""End-to-End Integration Tests with Test Data Isolation"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import pytest
import json
import asyncio
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock

# Import test isolation system
from test_environment_config import isolated_test_environment


class TestEndToEndWorkflowsIsolated:
    
    def setup_method(self, method):
        """Clean up before each test"""
        from fastmcp.task_management.infrastructure.database.database_config import get_db_config
        from sqlalchemy import text
        
        db_config = get_db_config()
        with db_config.get_session() as session:
            # Clean test data but preserve defaults
            try:
                session.execute(text("DELETE FROM tasks WHERE id LIKE 'test-%'"))
                session.execute(text("DELETE FROM projects WHERE id LIKE 'test-%' AND id != 'default_project'"))
                session.commit()
            except:
                session.rollback()

    """Test complete end-to-end workflows with test isolation"""
    
    @pytest.mark.isolated
    def test_complete_project_workflow(self):
        """Test complete project workflow from creation to completion"""
        with isolated_test_environment(test_id="e2e_project_workflow") as config:
            # Mock the complete project workflow
            workflow_state = {
                "projects": {},
                "tasks": {},
                "agents": {},
                "subtasks": {},
                "workflow_log": []
            }
            
            # Step 1: Create project
            def create_project(project_id, name, description):
                workflow_state["workflow_log"].append(f"Creating project: {project_id}")
                workflow_state["projects"][project_id] = {
                    "id": project_id,
                    "name": name,
                    "description": description,
                    "status": "active",
                    "git_branchs": {"main": {"name": "Main", "description": "Main tasks"}},
                    "registered_agents": {},
                    "agent_assignments": {}
                }
                return workflow_state["projects"][project_id]
            
            # Step 2: Register agents
            def register_agent(agent_id, name, capabilities):
                workflow_state["workflow_log"].append(f"Registering agent: {agent_id}")
                workflow_state["agents"][agent_id] = {
                    "id": agent_id,
                    "name": name,
                    "capabilities": capabilities,
                    "status": "active"
                }
                return workflow_state["agents"][agent_id]
            
            # Step 3: Assign agents to project
            def assign_agent_to_project(project_id, agent_id, git_branch_id="main"):
                workflow_state["workflow_log"].append(f"Assigning {agent_id} to {project_id}")
                project = workflow_state["projects"][project_id]
                agent = workflow_state["agents"][agent_id]
                
                project["registered_agents"][agent_id] = agent
                if git_branch_id not in project["agent_assignments"]:
                    project["agent_assignments"][git_branch_id] = []
                project["agent_assignments"][git_branch_id].append(agent_id)
            
            # Step 4: Create tasks
            def create_task(task_id, title, description, assignees, project_id):
                workflow_state["workflow_log"].append(f"Creating task: {task_id}")
                workflow_state["tasks"][task_id] = {
                    "id": task_id,
                    "title": title,
                    "description": description,
                    "status": "todo",
                    "assignees": assignees,
                    "project_id": project_id,
                    "dependencies": [],
                    "subtasks": []
                }
                return workflow_state["tasks"][task_id]
            
            # Step 5: Create subtasks
            def create_subtask(id, parent_task_id, title, description):
                workflow_state["workflow_log"].append(f"Creating subtask: {id}")
                workflow_state["subtasks"][id] = {
                    "id": id,
                    "parent_task_id": parent_task_id,
                    "title": title,
                    "description": description,
                    "completed": False
                }
                workflow_state["tasks"][parent_task_id]["subtasks"].append(id)
                return workflow_state["subtasks"][id]
            
            # Step 6: Execute workflow
            def execute_workflow():
                workflow_state["workflow_log"].append("Starting workflow execution")
                
                # Simulate task progression
                for task_id, task in workflow_state["tasks"].items():
                    task["status"] = "in_progress"
                    workflow_state["workflow_log"].append(f"Task {task_id} started")
                    
                    # Progress subtasks
                    for subtask_id in task["subtasks"]:
                        if subtask_id in workflow_state["subtasks"]:
                            subtask = workflow_state["subtasks"][subtask_id]
                            subtask["status"] = "done"
                            subtask["completed"] = True
                            workflow_state["workflow_log"].append(f"Subtask {subtask_id} completed")
                    
                    # Complete task when all subtasks done
                    all_subtasks_done = all(
                        workflow_state["subtasks"][sid]["completed"] 
                        for sid in task["subtasks"] 
                        if sid in workflow_state["subtasks"]
                    )
                    
                    if all_subtasks_done:
                        task["status"] = "done"
                        workflow_state["workflow_log"].append(f"Task {task_id} completed")
            
            # Execute the complete workflow
            
            # 1. Create project
            project = create_project("test_project", "Test Project", "End-to-end test project")
            assert project["id"] == "test_project"
            assert project["status"] == "active"
            
            # 2. Register agents
            coding_agent = register_agent("coding_agent", "Coding Agent", ["coding", "testing"])
            design_agent = register_agent("design_agent", "Design Agent", ["ui_design", "ux"])
            
            assert len(workflow_state["agents"]) == 2
            assert coding_agent["capabilities"] == ["coding", "testing"]
            
            # 3. Assign agents
            assign_agent_to_project("test_project", "coding_agent")
            assign_agent_to_project("test_project", "design_agent")
            
            project = workflow_state["projects"]["test_project"]
            assert len(project["registered_agents"]) == 2
            assert "coding_agent" in project["agent_assignments"]["main"]
            
            # 4. Create tasks
            task1 = create_task("task_001", "Implement login", "Create login functionality", 
                              ["coding_agent"], "test_project")
            task2 = create_task("task_002", "Design UI", "Create user interface", 
                              ["design_agent"], "test_project")
            
            assert len(workflow_state["tasks"]) == 2
            assert task1["assignees"] == ["coding_agent"]
            
            # 5. Create subtasks
            subtask1 = create_subtask("sub_001", "task_001", "Create login form", "HTML form")
            subtask2 = create_subtask("sub_002", "task_001", "Add validation", "Form validation")
            subtask3 = create_subtask("sub_003", "task_002", "Design mockups", "UI mockups")
            
            assert len(workflow_state["subtasks"]) == 3
            assert len(workflow_state["tasks"]["task_001"]["subtasks"]) == 2
            
            # 6. Execute workflow
            execute_workflow()
            
            # Verify workflow completion
            assert all(task["status"] == "done" for task in workflow_state["tasks"].values())
            assert all(subtask["completed"] for subtask in workflow_state["subtasks"].values())
            assert len(workflow_state["workflow_log"]) > 10  # Multiple workflow steps
            
            print("✅ Complete project workflow test passed")
    
    @pytest.mark.isolated
    def test_multi_agent_collaboration(self):
        """Test multi-agent collaboration on shared tasks"""
        with isolated_test_environment(test_id="multi_agent_collab") as config:
            # Test multi-agent collaboration
            agents = {"frontend": "active", "backend": "active", "devops": "active"}
            
            # Simulate collaboration
            collaboration_result = {"status": "success", "agents_collaborated": len(agents)}
            
            assert collaboration_result["status"] == "success"
            assert collaboration_result["agents_collaborated"] == 3
            
            print("✅ Multi-agent collaboration test passed")
    
    @pytest.mark.isolated
    def test_error_recovery_workflow(self):
        """Test error recovery and workflow resilience"""
        with isolated_test_environment(test_id="error_recovery_workflow") as config:
            # Mock error recovery system
            recovery_state = {
                "workflow_steps": [],
                "errors": [],
                "recovery_actions": [],
                "system_health": "healthy"
            }
            
            # Workflow step execution with error handling
            def execute_step_with_recovery(step_name, step_function, recovery_function=None):
                recovery_state["workflow_steps"].append(f"Executing: {step_name}")
                
                try:
                    # Simulate step execution
                    result = step_function()
                    recovery_state["workflow_steps"].append(f"Completed: {step_name}")
                    return result
                except Exception as e:
                    # Log error
                    error_info = {
                        "step": step_name,
                        "error": str(e),
                        "timestamp": "2025-01-26T12:00:00Z"
                    }
                    recovery_state["errors"].append(error_info)
                    recovery_state["workflow_steps"].append(f"Error in: {step_name}")
                    
                    # Attempt recovery
                    if recovery_function:
                        recovery_state["recovery_actions"].append(f"Attempting recovery for: {step_name}")
                        try:
                            recovery_result = recovery_function()
                            recovery_state["recovery_actions"].append(f"Recovery successful for: {step_name}")
                            return recovery_result
                        except Exception as recovery_error:
                            recovery_state["recovery_actions"].append(f"Recovery failed for: {step_name}")
                            recovery_state["system_health"] = "degraded"
                            raise recovery_error
                    else:
                        recovery_state["system_health"] = "degraded"
                        raise e
            
            # Define workflow steps (some will fail)
            def step_1_database_setup():
                # This step succeeds
                return {"status": "success", "database": "connected"}
            
            def step_2_api_initialization():
                # This step fails
                raise Exception("API initialization failed - port already in use")
            
            def step_2_recovery():
                # Recovery: try different port
                return {"status": "recovered", "api": "started_on_alternate_port"}
            
            def step_3_load_configuration():
                # This step fails
                raise Exception("Configuration file not found")
            
            def step_3_recovery():
                # Recovery: create default configuration
                return {"status": "recovered", "config": "default_config_created"}
            
            def step_4_start_services():
                # This step succeeds
                return {"status": "success", "services": "all_started"}
            
            def step_5_health_check():
                # This step fails without recovery
                raise Exception("Health check failed - external service unavailable")
            
            # Execute workflow with error recovery
            workflow_results = []
            
            # Step 1: Database setup (succeeds)
            result1 = execute_step_with_recovery("database_setup", step_1_database_setup)
            workflow_results.append(result1)
            assert result1["status"] == "success"
            
            # Step 2: API initialization (fails, recovers)
            result2 = execute_step_with_recovery("api_initialization", step_2_api_initialization, step_2_recovery)
            workflow_results.append(result2)
            assert result2["status"] == "recovered"
            
            # Step 3: Load configuration (fails, recovers)
            result3 = execute_step_with_recovery("load_configuration", step_3_load_configuration, step_3_recovery)
            workflow_results.append(result3)
            assert result3["status"] == "recovered"
            
            # Step 4: Start services (succeeds)
            result4 = execute_step_with_recovery("start_services", step_4_start_services)
            workflow_results.append(result4)
            assert result4["status"] == "success"
            
            # Step 5: Health check (fails, no recovery)
            try:
                result5 = execute_step_with_recovery("health_check", step_5_health_check)
                workflow_results.append(result5)
            except Exception as e:
                # Expected failure
                assert "Health check failed" in str(e)
            
            # Verify error recovery results
            assert len(recovery_state["errors"]) == 3  # 3 steps failed
            assert len(recovery_state["recovery_actions"]) == 4  # 2 successful recoveries + 2 logs
            assert recovery_state["system_health"] == "degraded"  # Final health check failed
            
            # Verify workflow progression
            assert len(workflow_results) == 4  # 4 steps completed (1 failed completely)
            assert sum(1 for r in workflow_results if r["status"] == "success") == 2
            assert sum(1 for r in workflow_results if r["status"] == "recovered") == 2
            
            # Verify error logging
            error_steps = [error["step"] for error in recovery_state["errors"]]
            assert "api_initialization" in error_steps
            assert "load_configuration" in error_steps
            assert "health_check" in error_steps
            
            print("✅ Error recovery workflow test passed")
    
    @pytest.mark.isolated
    def test_performance_under_load(self):
        """Test system performance under load conditions"""
        with isolated_test_environment(test_id="performance_load") as config:
            # Mock performance testing system
            performance_state = {
                "requests": [],
                "response_times": [],
                "errors": [],
                "resource_usage": [],
                "concurrent_connections": 0
            }
            
            import time
            import random
            
            # Simulate request processing
            def process_request(request_id, request_type, payload_size=1000):
                start_time = time.time()
                
                # Simulate processing time based on request type
                processing_times = {
                    "simple": 0.001,  # 1ms
                    "complex": 0.005,  # 5ms
                    "heavy": 0.010     # 10ms
                }
                
                base_time = processing_times.get(request_type, 0.001)
                # Add some randomness
                actual_time = base_time + random.uniform(0, base_time * 0.5)
                
                time.sleep(actual_time)
                
                end_time = time.time()
                response_time = (end_time - start_time) * 1000  # Convert to ms
                
                # Record metrics
                performance_state["requests"].append({
                    "id": request_id,
                    "type": request_type,
                    "payload_size": payload_size,
                    "response_time": response_time,
                    "timestamp": time.time()
                })
                
                performance_state["response_times"].append(response_time)
                
                # Simulate occasional errors under load
                if len(performance_state["requests"]) > 50 and random.random() < 0.05:  # 5% error rate
                    error = {
                        "request_id": request_id,
                        "error": "Timeout under load",
                        "timestamp": time.time()
                    }
                    performance_state["errors"].append(error)
                    raise Exception("Request timeout")
                
                return {"status": "success", "response_time": response_time}
            
            # Simulate concurrent connections
            def simulate_concurrent_load(num_requests=100):
                import threading
                import queue
                
                results = queue.Queue()
                
                def worker(request_id):
                    try:
                        performance_state["concurrent_connections"] += 1
                        
                        # Vary request types
                        request_types = ["simple", "simple", "complex", "heavy"]  # Weighted distribution
                        request_type = random.choice(request_types)
                        
                        result = process_request(request_id, request_type)
                        results.put(("success", result))
                    except Exception as e:
                        results.put(("error", str(e)))
                    finally:
                        performance_state["concurrent_connections"] -= 1
                
                # Start threads
                threads = []
                for i in range(num_requests):
                    thread = threading.Thread(target=worker, args=(f"req_{i}",))
                    threads.append(thread)
                    thread.start()
                
                # Wait for completion
                for thread in threads:
                    thread.join()
                
                # Collect results
                success_count = 0
                error_count = 0
                
                while not results.empty():
                    result_type, result_data = results.get()
                    if result_type == "success":
                        success_count += 1
                    else:
                        error_count += 1
                
                return success_count, error_count
            
            # Calculate performance metrics
            def calculate_metrics():
                if not performance_state["response_times"]:
                    return {}
                
                response_times = performance_state["response_times"]
                
                return {
                    "total_requests": len(performance_state["requests"]),
                    "total_errors": len(performance_state["errors"]),
                    "error_rate": len(performance_state["errors"]) / len(performance_state["requests"]) * 100,
                    "avg_response_time": sum(response_times) / len(response_times),
                    "min_response_time": min(response_times),
                    "max_response_time": max(response_times),
                    "p95_response_time": sorted(response_times)[int(len(response_times) * 0.95)],
                    "requests_per_second": len(response_times) / 10  # Assuming 10 second test window
                }
            
            # Run performance test
            print("🔄 Running performance test with 100 concurrent requests...")
            
            success_count, error_count = simulate_concurrent_load(100)
            
            # Calculate and verify metrics
            metrics = calculate_metrics()
            
            # Performance assertions
            assert metrics["total_requests"] == 100
            assert metrics["error_rate"] < 10  # Less than 10% error rate
            assert metrics["avg_response_time"] < 50  # Less than 50ms average
            assert metrics["requests_per_second"] > 5  # At least 5 RPS
            
            # Verify load handling
            assert success_count >= 90  # At least 90% success rate
            assert error_count <= 10   # At most 10% errors
            
            # Verify no memory leaks (connections cleaned up)
            assert performance_state["concurrent_connections"] == 0
            
            print(f"✅ Performance test passed:")
            print(f"   - Total requests: {metrics['total_requests']}")
            print(f"   - Success rate: {(success_count/100)*100:.1f}%")
            print(f"   - Average response time: {metrics['avg_response_time']:.2f}ms")
            print(f"   - Requests per second: {metrics['requests_per_second']:.1f}")
            
            print("✅ Performance under load test passed")


# Run tests if executed directly
if __name__ == "__main__":
    test_instance = TestEndToEndWorkflowsIsolated()
    
    print("🧪 Running end-to-end integration tests...")
    
    test_instance.test_complete_project_workflow()
    test_instance.test_multi_agent_collaboration()
    test_instance.test_error_recovery_workflow()
    test_instance.test_performance_under_load()
    
    print("🎉 All integration tests passed!") 